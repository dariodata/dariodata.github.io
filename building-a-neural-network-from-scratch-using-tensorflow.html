<!doctype html>
<html lang="en" itemscope itemtype="http://schema.org/Person">
<head>
  <meta charset="utf-8">
  <!-- Site Meta Data -->
  <title>Building a neural network from scratch using TensorFlow</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="">
  <meta name="author" content="Dario Arcos-Díaz">

  <link rel="shortcut icon" href="">

  <!-- schema.org -->
  <meta itemprop="name" content="Dario Arcos-Díaz">
  <meta itemprop="image" content="">
  <meta itemprop="description" content="">

  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,600,700' rel='stylesheet' type='text/css'>
  <!-- Style Meta Data -->
  <link rel="stylesheet" href="/theme/css/style.css" type="text/css" />
  <link rel="stylesheet" href="/theme/css/pygments.css" type="text/css" />

  <!-- Feed Meta Data -->

  <!-- Twitter Feed -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:site" content="">
  <meta name="twitter:image" content="">

<meta name="twitter:creator" content="">
<meta name="twitter:url" content="/building-a-neural-network-from-scratch-using-tensorflow.html">
<meta name="twitter:title" content="Dario Arcos-Díaz ~ Building a neural network from scratch using TensorFlow">
<meta name="twitter:description" content="<h2>MNIST-data set digit recognition</h2>
<p>I recently discovered Andrew Ng's new Deep Learning specialization on coursera and dove right in. His teaching style is an absolute match for me: start with the basics, understanding every detail and then move on gradually to more and more complex topics. It's not for everyone …</p>">

<!-- Facebook Meta Data -->
<meta property="og:title" content="Dario Arcos-Díaz ~ Building a neural network from scratch using TensorFlow" />
<meta property="og:description" content="<h2>MNIST-data set digit recognition</h2>
<p>I recently discovered Andrew Ng's new Deep Learning specialization on coursera and dove right in. His teaching style is an absolute match for me: start with the basics, understanding every detail and then move on gradually to more and more complex topics. It's not for everyone …</p>" />
<meta property="og:image" content="" />
</head>

<body>
  <!-- Sidebar -->
  <aside>
    <!--<center><a href=""><img id="avatar" src=""></a></center>-->
    <h1>Dario Arcos-Díaz</h1>
      <p>Neuroscientist, Project Manager, and Data Enthusiast</p>
    <br>


    <nav class="nav">
      <ul class="list-bare">
      
          <li><a class="nav__link" href="/index.html">Home</a></li>
          <li><a class="nav__link" href="/pages/about.html">About</a></li>
          <li><a class="nav__link" href="/archives.html">Archive</a></li>
         
         
      </ul>
    </nav>

    <p class="social">
        <a href="https://github.com/dariodata" target="_blank" ><img src="/theme/images/icons/github.png"></a>
        <a href="https://www.linkedin.com/in/arcosdiaz" target="_blank" ><img src="/theme/images/icons/linkedin.png"></a>
    </p>

    <!--
    <h2>Categories</h2>
    <ul class="navbar">
      <li class="active"><a href="/category/machine-learning.html">Machine Learning</a></li>
      <li><a href="/category/simulation.html">Simulation</a></li>
      <li><a href="/category/visualization.html">Visualization</a></li>
    </ul> 
    -->
  </aside>

  <!-- Content -->
  <article>
<section id="content">
    <article>
        <h2 class="post_title post_detail"><a href="/building-a-neural-network-from-scratch-using-tensorflow.html" rel="bookmark" title="Permalink to Building a neural network from scratch using TensorFlow">Building a neural network from scratch using TensorFlow</a></h2>
        <div class="entry-content blog-post">
            <h2>MNIST-data set digit recognition</h2>
<p>I recently discovered Andrew Ng's new Deep Learning specialization on coursera and dove right in. His teaching style is an absolute match for me: start with the basics, understanding every detail and then move on gradually to more and more complex topics. It's not for everyone, I agree, as many people do not have the patience to learn how the car works before learning to drive. But driving while thinking it's all just magic is half as fun.</p>
<p>Part of the specialization is learning to use TensorFlow. A dream come true, as I have so far always used deep learning frameworks like keras, which use TensorFlow in the backend, and I always wanted to get to code in the real thing. Don't take me wrong, keras is amazing! It makes building neural networks really easy. But using TensorFlow you really have to <em>think</em> about what you are doing in order for it to work. And that means that you learn much more!</p>
<p>So let's get to it. I decided to build a first simple neural network using TensorFlow to solve the classic MNIST digit recognition problem.</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">h5py</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>

<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="kn">import</span> <span class="n">ops</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>


<h2>Loading the data</h2>
<p>The set consists of 28x28pixels grayscale pictures of handdrawn digits. There are 42000 pictures in the train set and 28000 in the test set. Only the train set has corresponding labels (digit encoded from 0 to 9).</p>
<div class="highlight"><pre><span></span><span class="c1"># Loading the dataset</span>
<span class="c1">#X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()</span>
<span class="n">train_orig</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;train.csv&#39;</span><span class="p">)</span>
<span class="n">test_orig</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;test.csv&#39;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;Train set shape:&#39;</span><span class="p">,</span>  <span class="n">train_orig</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;Test set shape:&#39;</span><span class="p">,</span>  <span class="n">test_orig</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>Train set shape: (42000, 785)
Test set shape: (28000, 784)
</pre></div>


<p>I now separate the labels from the features in the train set to get X_train_orig and Y_train_orig.</p>
<div class="highlight"><pre><span></span><span class="n">X_train_orig</span> <span class="o">=</span> <span class="n">train_orig</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:]</span>
<span class="n">Y_train_orig</span> <span class="o">=</span> <span class="n">train_orig</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span>
<span class="n">X_test_orig</span> <span class="o">=</span> <span class="n">test_orig</span>

<span class="n">X_train_orig</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">Y_train_orig</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">X_test_orig</span><span class="o">.</span><span class="n">shape</span>
</pre></div>


<div class="highlight"><pre><span></span>((42000, 784), (42000,), (28000, 784))
</pre></div>


<p>Since I do not have the labels of the test set, but I do need to estimate how well my neural network is performing, I decided to split my train set in proper train and validation sets. I use 20% of the train data as validation to check for accuracy later in the script.</p>
<div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">Y_val</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_train_orig</span><span class="p">,</span> <span class="n">Y_train_orig</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.20</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">X_val</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">Y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">Y_val</span><span class="o">.</span><span class="n">shape</span>
</pre></div>


<div class="highlight"><pre><span></span>((33600, 784), (8400, 784), (33600,), (8400,))
</pre></div>


<p>The data doesn't need too much preprocessing, as the images have already been flattened out into vectors. I only perform normalization of the grayscale values and one-hot encode the Y labels.</p>
<div class="highlight"><pre><span></span><span class="c1"># Normalize features</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">T</span><span class="o">/</span><span class="mf">255.</span>
<span class="n">X_val</span> <span class="o">=</span> <span class="n">X_val</span><span class="o">.</span><span class="n">T</span><span class="o">/</span><span class="mf">255.</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">X_test_orig</span><span class="o">.</span><span class="n">T</span><span class="o">/</span><span class="mf">255.</span>
<span class="c1"># One-hot encode labels</span>
<span class="n">Y_train</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">Y_train</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
<span class="n">Y_val</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">Y_val</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="k">print</span> <span class="p">(</span><span class="s2">&quot;number of training examples = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
<span class="k">print</span> <span class="p">(</span><span class="s2">&quot;number of validation examples = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">X_val</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
<span class="k">print</span> <span class="p">(</span><span class="s2">&quot;number of test examples = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
<span class="k">print</span> <span class="p">(</span><span class="s2">&quot;X_train shape: &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="k">print</span> <span class="p">(</span><span class="s2">&quot;X_val shape: &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">X_val</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="k">print</span> <span class="p">(</span><span class="s2">&quot;Y_train shape: &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">Y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="k">print</span> <span class="p">(</span><span class="s2">&quot;Y_val shape: &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">Y_val</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="k">print</span> <span class="p">(</span><span class="s2">&quot;X_test shape: &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
</pre></div>


<div class="highlight"><pre><span></span>number of training examples = 33600
number of validation examples = 8400
number of test examples = 28000
X_train shape: (784, 33600)
X_val shape: (784, 8400)
Y_train shape: (10, 33600)
Y_val shape: (10, 8400)
X_test shape: (784, 28000)
</pre></div>


<h1>Define helper functions</h1>
<p>Now it's time to define some useful functions that can be later put together easily when building the deep learning model.</p>
<h3>Create placeholders for data input</h3>
<p>These are the placeholders where the input data will be later entered into the model.</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">create_placeholders</span><span class="p">(</span><span class="n">n_x</span><span class="p">,</span> <span class="n">n_y</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates the placeholders for the tensorflow session.</span>

<span class="sd">    Arguments:</span>
<span class="sd">    n_x -- scalar, size of an image vector (num_px * num_px = 64 * 64 * 3 = 12288)</span>
<span class="sd">    n_y -- scalar, number of classes (from 0 to 5, so -&gt; 6)</span>

<span class="sd">    Returns:</span>
<span class="sd">    X -- placeholder for the data input, of shape [n_x, None] and dtype &quot;float&quot;</span>
<span class="sd">    Y -- placeholder for the input labels, of shape [n_y, None] and dtype &quot;float&quot;</span>

<span class="sd">    Tips:</span>
<span class="sd">    - You will use None because it let&#39;s us be flexible on the number of examples you will for the placeholders.</span>
<span class="sd">      In fact, the number of examples during test/train is different.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">X</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">n_x</span><span class="p">,</span> <span class="bp">None</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;X&#39;</span><span class="p">)</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">n_y</span><span class="p">,</span> <span class="bp">None</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;Y&#39;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span>
</pre></div>


<h3>Initialize parameters</h3>
<p>Before starting learning, we initialize the weights of the network appropriately. This is where the number of layers and the number of hidden units is defined. In this case I will build a 3-layered network with 100, and 50 hidden units, plus 10 output units (for the 0-9 digit classes).</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">initialize_parameters</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initializes parameters to build a neural network with tensorflow.</span>

<span class="sd">    Returns:</span>
<span class="sd">    parameters -- a dictionary of tensors containing W1, b1, W2, b2, W3, b3</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">tf</span><span class="o">.</span><span class="n">set_random_seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">W1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s1">&#39;W1&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">784</span><span class="p">],</span> <span class="n">initializer</span><span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">xavier_initializer</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">b1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s1">&#39;b1&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="n">initializer</span><span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_initializer</span><span class="p">())</span>
    <span class="n">W2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s1">&#39;W2&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">50</span><span class="p">,</span><span class="mi">100</span><span class="p">],</span> <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">xavier_initializer</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">b2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s1">&#39;b2&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">50</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="n">initializer</span><span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_initializer</span><span class="p">())</span>
    <span class="n">W3</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s1">&#39;W3&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span><span class="mi">50</span><span class="p">],</span> <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">xavier_initializer</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">b3</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s1">&#39;b3&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="n">initializer</span><span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_initializer</span><span class="p">())</span>

    <span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;W1&quot;</span><span class="p">:</span> <span class="n">W1</span><span class="p">,</span>
                  <span class="s2">&quot;b1&quot;</span><span class="p">:</span> <span class="n">b1</span><span class="p">,</span>
                  <span class="s2">&quot;W2&quot;</span><span class="p">:</span> <span class="n">W2</span><span class="p">,</span>
                  <span class="s2">&quot;b2&quot;</span><span class="p">:</span> <span class="n">b2</span><span class="p">,</span>
                  <span class="s2">&quot;W3&quot;</span><span class="p">:</span> <span class="n">W3</span><span class="p">,</span>
                  <span class="s2">&quot;b3&quot;</span><span class="p">:</span> <span class="n">b3</span><span class="p">}</span>

    <span class="k">return</span> <span class="n">parameters</span>
</pre></div>


<h3>Define forward propagation</h3>
<p>The key to building the model in TensorFlow is to define the forward propagation computation. </p>
<p>There are three basic steps to be performed per layer:
1) Calculate the Z vectors by matrix multiplication of weights and input from previous layer
2) Calculate the activation (A) vectors by using an activation function, in this case relu
3) Optional (but in this case essential): apply dropout where needed in order to introduce regularization in the network and prevent overfitting.</p>
<p>Only the Z vector of the output layer we leave as is without the activation function.</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward_propagation</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implements the forward propagation for the model: LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SOFTMAX</span>

<span class="sd">    Arguments:</span>
<span class="sd">    X -- input dataset placeholder, of shape (input size, number of examples)</span>
<span class="sd">    parameters -- python dictionary containing your parameters &quot;W1&quot;, &quot;b1&quot;, &quot;W2&quot;, &quot;b2&quot;, &quot;W3&quot;, &quot;b3&quot;</span>
<span class="sd">                  the shapes are given in initialize_parameters</span>

<span class="sd">    Returns:</span>
<span class="sd">    Z3 -- the output of the last LINEAR unit</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Retrieve the parameters from the dictionary &quot;parameters&quot; </span>
    <span class="n">W1</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;W1&#39;</span><span class="p">]</span>
    <span class="n">b1</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;b1&#39;</span><span class="p">]</span>
    <span class="n">W2</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;W2&#39;</span><span class="p">]</span>
    <span class="n">b2</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;b2&#39;</span><span class="p">]</span>
    <span class="n">W3</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;W3&#39;</span><span class="p">]</span>
    <span class="n">b3</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;b3&#39;</span><span class="p">]</span>

    <span class="n">Z1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">W1</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span>                    <span class="c1"># Z1 = np.dot(W1, X) + b1</span>
    <span class="n">A1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">Z1</span><span class="p">)</span>                           <span class="c1"># A1 = relu(Z1)</span>
    <span class="n">A1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">A1</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">)</span>
    <span class="n">Z2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">W2</span><span class="p">,</span> <span class="n">A1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2</span>                   <span class="c1"># Z2 = np.dot(W2, a1) + b2</span>
    <span class="n">A2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">Z2</span><span class="p">)</span>                           <span class="c1"># A2 = relu(Z2)</span>
    <span class="n">A2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">A2</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">)</span>
    <span class="n">Z3</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">W3</span><span class="p">,</span> <span class="n">A2</span><span class="p">)</span> <span class="o">+</span> <span class="n">b3</span>                   <span class="c1"># Z3 = np.dot(W3,Z2) + b3</span>

    <span class="k">return</span> <span class="n">Z3</span>
</pre></div>


<p>The output of the last layer (Z3) will be used to compute the cost of the model using softmax cross entropy function.</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compute_cost</span><span class="p">(</span><span class="n">Z3</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the cost</span>

<span class="sd">    Arguments:</span>
<span class="sd">    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)</span>
<span class="sd">    Y -- &quot;true&quot; labels vector placeholder, same shape as Z3</span>

<span class="sd">    Returns:</span>
<span class="sd">    cost - Tensor of the cost function</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">Z3</span><span class="p">)</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>

    <span class="n">cost</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">cost</span>
</pre></div>


<p>Now we need a mini-batch generation function, in order to use mini-batch gradient descent instead of batch gradient descent.</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">random_mini_batches</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">mini_batch_size</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span> <span class="n">seed</span> <span class="o">=</span> <span class="mi">0</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates a list of random minibatches from (X, Y)</span>

<span class="sd">    Arguments:</span>
<span class="sd">    X -- input data, of shape (input size, number of examples)</span>
<span class="sd">    Y -- true &quot;label&quot; vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)</span>
<span class="sd">    mini_batch_size - size of the mini-batches, integer</span>
<span class="sd">    seed</span>

<span class="sd">    Returns:</span>
<span class="sd">    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">m</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>                  <span class="c1"># number of training examples</span>
    <span class="n">mini_batches</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="c1"># Step 1: Shuffle (X, Y)</span>
    <span class="n">permutation</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">m</span><span class="p">))</span>
    <span class="n">shuffled_X</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="n">permutation</span><span class="p">]</span>
    <span class="n">shuffled_Y</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[:,</span> <span class="n">permutation</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">m</span><span class="p">))</span>

    <span class="c1"># Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.</span>
    <span class="n">num_complete_minibatches</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">m</span><span class="o">/</span><span class="n">mini_batch_size</span><span class="p">)</span> <span class="c1"># number of mini batches of size mini_batch_size in your partitionning</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_complete_minibatches</span><span class="p">):</span>
        <span class="n">mini_batch_X</span> <span class="o">=</span> <span class="n">shuffled_X</span><span class="p">[:,</span> <span class="n">k</span> <span class="o">*</span> <span class="n">mini_batch_size</span> <span class="p">:</span> <span class="n">k</span> <span class="o">*</span> <span class="n">mini_batch_size</span> <span class="o">+</span> <span class="n">mini_batch_size</span><span class="p">]</span>
        <span class="n">mini_batch_Y</span> <span class="o">=</span> <span class="n">shuffled_Y</span><span class="p">[:,</span> <span class="n">k</span> <span class="o">*</span> <span class="n">mini_batch_size</span> <span class="p">:</span> <span class="n">k</span> <span class="o">*</span> <span class="n">mini_batch_size</span> <span class="o">+</span> <span class="n">mini_batch_size</span><span class="p">]</span>
        <span class="n">mini_batch</span> <span class="o">=</span> <span class="p">(</span><span class="n">mini_batch_X</span><span class="p">,</span> <span class="n">mini_batch_Y</span><span class="p">)</span>
        <span class="n">mini_batches</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mini_batch</span><span class="p">)</span>

    <span class="c1"># Handling the end case (last mini-batch &lt; mini_batch_size)</span>
    <span class="k">if</span> <span class="n">m</span> <span class="o">%</span> <span class="n">mini_batch_size</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">mini_batch_X</span> <span class="o">=</span> <span class="n">shuffled_X</span><span class="p">[:,</span> <span class="n">num_complete_minibatches</span> <span class="o">*</span> <span class="n">mini_batch_size</span> <span class="p">:</span> <span class="n">m</span><span class="p">]</span>
        <span class="n">mini_batch_Y</span> <span class="o">=</span> <span class="n">shuffled_Y</span><span class="p">[:,</span> <span class="n">num_complete_minibatches</span> <span class="o">*</span> <span class="n">mini_batch_size</span> <span class="p">:</span> <span class="n">m</span><span class="p">]</span>
        <span class="n">mini_batch</span> <span class="o">=</span> <span class="p">(</span><span class="n">mini_batch_X</span><span class="p">,</span> <span class="n">mini_batch_Y</span><span class="p">)</span>
        <span class="n">mini_batches</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mini_batch</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">mini_batches</span>
</pre></div>


<p>To make the predictions we create another function that doesn't include dropout.</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">parameters</span><span class="p">):</span>

    <span class="n">W1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;W1&quot;</span><span class="p">])</span>
    <span class="n">b1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;b1&quot;</span><span class="p">])</span>
    <span class="n">W2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;W2&quot;</span><span class="p">])</span>
    <span class="n">b2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;b2&quot;</span><span class="p">])</span>
    <span class="n">W3</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;W3&quot;</span><span class="p">])</span>
    <span class="n">b3</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;b3&quot;</span><span class="p">])</span>

    <span class="n">params</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;W1&quot;</span><span class="p">:</span> <span class="n">W1</span><span class="p">,</span>
              <span class="s2">&quot;b1&quot;</span><span class="p">:</span> <span class="n">b1</span><span class="p">,</span>
              <span class="s2">&quot;W2&quot;</span><span class="p">:</span> <span class="n">W2</span><span class="p">,</span>
              <span class="s2">&quot;b2&quot;</span><span class="p">:</span> <span class="n">b2</span><span class="p">,</span>
              <span class="s2">&quot;W3&quot;</span><span class="p">:</span> <span class="n">W3</span><span class="p">,</span>
              <span class="s2">&quot;b3&quot;</span><span class="p">:</span> <span class="n">b3</span><span class="p">}</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="s2">&quot;float&quot;</span><span class="p">,</span> <span class="p">[</span><span class="mi">784</span><span class="p">,</span> <span class="bp">None</span><span class="p">])</span>

    <span class="n">z3</span> <span class="o">=</span> <span class="n">forward_propagation_for_predict</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">z3</span><span class="p">)</span>

    <span class="n">sess</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span>
    <span class="n">prediction</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">feed_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">x</span><span class="p">:</span> <span class="n">X</span><span class="p">})</span>

    <span class="k">return</span> <span class="n">prediction</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward_propagation_for_predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">parameters</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implements the forward propagation for the model: LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SOFTMAX</span>

<span class="sd">    Arguments:</span>
<span class="sd">    X -- input dataset placeholder, of shape (input size, number of examples)</span>
<span class="sd">    parameters -- python dictionary containing your parameters &quot;W1&quot;, &quot;b1&quot;, &quot;W2&quot;, &quot;b2&quot;, &quot;W3&quot;, &quot;b3&quot;</span>
<span class="sd">                  the shapes are given in initialize_parameters</span>

<span class="sd">    Returns:</span>
<span class="sd">    Z3 -- the output of the last LINEAR unit</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Retrieve the parameters from the dictionary &quot;parameters&quot; </span>
    <span class="n">W1</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;W1&#39;</span><span class="p">]</span>
    <span class="n">b1</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;b1&#39;</span><span class="p">]</span>
    <span class="n">W2</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;W2&#39;</span><span class="p">]</span>
    <span class="n">b2</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;b2&#39;</span><span class="p">]</span>
    <span class="n">W3</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;W3&#39;</span><span class="p">]</span>
    <span class="n">b3</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;b3&#39;</span><span class="p">]</span> 
                                                           <span class="c1"># Numpy Equivalents:</span>
    <span class="n">Z1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">W1</span><span class="p">,</span> <span class="n">X</span><span class="p">),</span> <span class="n">b1</span><span class="p">)</span>                      <span class="c1"># Z1 = np.dot(W1, X) + b1</span>
    <span class="n">A1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">Z1</span><span class="p">)</span>                                    <span class="c1"># A1 = relu(Z1)</span>
    <span class="n">Z2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">W2</span><span class="p">,</span> <span class="n">A1</span><span class="p">),</span> <span class="n">b2</span><span class="p">)</span>                     <span class="c1"># Z2 = np.dot(W2, a1) + b2</span>
    <span class="n">A2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">Z2</span><span class="p">)</span>                                    <span class="c1"># A2 = relu(Z2)</span>
    <span class="n">Z3</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">W3</span><span class="p">,</span> <span class="n">A2</span><span class="p">),</span> <span class="n">b3</span><span class="p">)</span>                     <span class="c1"># Z3 = np.dot(W3,Z2) + b3</span>

    <span class="k">return</span> <span class="n">Z3</span>
</pre></div>


<h2>Build the model</h2>
<p>Now I'm ready to build the model putting the pieces together. One more thing to define is the optimization algorith to be used, in this case Adam optimization. In the end, I will get an estimate of the accuracy of the model, a plot of the loss along the epochs and the learned parameters of the network.</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">Y_test</span><span class="p">,</span> <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.0001</span><span class="p">,</span>
          <span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">700</span><span class="p">,</span> <span class="n">minibatch_size</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span> <span class="n">print_cost</span> <span class="o">=</span> <span class="bp">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implements a three-layer tensorflow neural network: LINEAR-&gt;RELU-&gt;LINEAR-&gt;RELU-&gt;LINEAR-&gt;SOFTMAX.</span>

<span class="sd">    Arguments:</span>
<span class="sd">    X_train -- training set, of shape (input size = 12288, number of training examples = 1080)</span>
<span class="sd">    Y_train -- test set, of shape (output size = 6, number of training examples = 1080)</span>
<span class="sd">    X_test -- training set, of shape (input size = 12288, number of training examples = 120)</span>
<span class="sd">    Y_test -- test set, of shape (output size = 6, number of test examples = 120)</span>
<span class="sd">    learning_rate -- learning rate of the optimization</span>
<span class="sd">    num_epochs -- number of epochs of the optimization loop</span>
<span class="sd">    minibatch_size -- size of a minibatch</span>
<span class="sd">    print_cost -- True to print the cost every 100 epochs</span>

<span class="sd">    Returns:</span>
<span class="sd">    parameters -- parameters learnt by the model. They can then be used to predict.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">ops</span><span class="o">.</span><span class="n">reset_default_graph</span><span class="p">()</span>                         <span class="c1"># to be able to rerun the model without overwriting tf variables</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">set_random_seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>                             <span class="c1"># to keep consistent results</span>
    <span class="n">seed</span> <span class="o">=</span> <span class="mi">3</span>                                          <span class="c1"># to keep consistent results</span>
    <span class="p">(</span><span class="n">n_x</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span>                          <span class="c1"># (n_x: input size, m : number of examples in the train set)</span>
    <span class="n">n_y</span> <span class="o">=</span> <span class="n">Y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>                            <span class="c1"># n_y : output size</span>
    <span class="n">costs</span> <span class="o">=</span> <span class="p">[]</span>                                        <span class="c1"># To keep track of the cost</span>

    <span class="c1"># Create Placeholders of shape (n_x, n_y)</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">create_placeholders</span><span class="p">(</span><span class="n">n_x</span><span class="p">,</span> <span class="n">n_y</span><span class="p">)</span>
    <span class="n">keep_prob</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;keep_prob&#39;</span><span class="p">)</span>

    <span class="c1"># Initialize parameters</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="n">initialize_parameters</span><span class="p">()</span>

    <span class="c1"># Forward propagation: Build the forward propagation in the tensorflow graph</span>
    <span class="n">Z3</span> <span class="o">=</span> <span class="n">forward_propagation</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">)</span>

    <span class="c1"># Cost function: Add cost function to tensorflow graph</span>
    <span class="n">cost</span> <span class="o">=</span> <span class="n">compute_cost</span><span class="p">(</span><span class="n">Z3</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>

    <span class="c1"># Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>

    <span class="c1"># Initialize all the variables</span>
    <span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>

    <span class="c1"># Start the session to compute the tensorflow graph</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>

        <span class="c1"># Run the initialization</span>
        <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">init</span><span class="p">)</span>

        <span class="c1"># Do the training loop</span>
        <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>

            <span class="n">epoch_cost</span> <span class="o">=</span> <span class="mf">0.</span>                       <span class="c1"># Defines a cost related to an epoch</span>
            <span class="n">num_minibatches</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">m</span> <span class="o">/</span> <span class="n">minibatch_size</span><span class="p">)</span> <span class="c1"># number of minibatches of size minibatch_size in the train set</span>
            <span class="n">seed</span> <span class="o">=</span> <span class="n">seed</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="n">minibatches</span> <span class="o">=</span> <span class="n">random_mini_batches</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">minibatch_size</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span>

            <span class="k">for</span> <span class="n">minibatch</span> <span class="ow">in</span> <span class="n">minibatches</span><span class="p">:</span>

                <span class="c1"># Select a minibatch</span>
                <span class="p">(</span><span class="n">minibatch_X</span><span class="p">,</span> <span class="n">minibatch_Y</span><span class="p">)</span> <span class="o">=</span> <span class="n">minibatch</span>

                <span class="c1"># IMPORTANT: The line that runs the graph on a minibatch.</span>
                <span class="c1"># Run the session to execute the &quot;optimizer&quot; and the &quot;cost&quot;, the feedict should contain a minibatch for (X,Y).</span>
                <span class="c1">### START CODE HERE ### (1 line)</span>
                <span class="n">_</span> <span class="p">,</span> <span class="n">minibatch_cost</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">cost</span><span class="p">],</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">minibatch_X</span><span class="p">,</span> <span class="n">Y</span><span class="p">:</span><span class="n">minibatch_Y</span><span class="p">,</span> 
                                                                            <span class="n">keep_prob</span><span class="p">:</span><span class="mf">0.8</span><span class="p">})</span>
                <span class="c1">### END CODE HERE ###</span>

                <span class="n">epoch_cost</span> <span class="o">+=</span> <span class="n">minibatch_cost</span> <span class="o">/</span> <span class="n">num_minibatches</span>

            <span class="c1"># Print the cost every epoch</span>
            <span class="k">if</span> <span class="n">print_cost</span> <span class="o">==</span> <span class="bp">True</span> <span class="ow">and</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Cost after epoch </span><span class="si">%i</span><span class="s2">: </span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">epoch_cost</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">print_cost</span> <span class="o">==</span> <span class="bp">True</span> <span class="ow">and</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">5</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">costs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">epoch_cost</span><span class="p">)</span>

        <span class="c1"># plot the cost</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">costs</span><span class="p">))</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;cost&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;iterations (per tens)&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Learning rate =&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">))</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

        <span class="c1"># lets save the parameters in a variable</span>
        <span class="n">parameters</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">parameters</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Parameters have been trained!&quot;</span><span class="p">)</span>

        <span class="c1"># Calculate the correct predictions</span>
        <span class="n">correct_prediction</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">Z3</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">Y</span><span class="p">))</span>

        <span class="c1"># Calculate accuracy on the test set</span>
        <span class="n">accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">correct_prediction</span><span class="p">,</span> <span class="s2">&quot;float&quot;</span><span class="p">))</span>

        <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Train Accuracy:&quot;</span><span class="p">,</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">eval</span><span class="p">({</span><span class="n">X</span><span class="p">:</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">Y</span><span class="p">:</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">:</span><span class="mf">1.0</span><span class="p">}))</span>
        <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Test Accuracy:&quot;</span><span class="p">,</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">eval</span><span class="p">({</span><span class="n">X</span><span class="p">:</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">Y</span><span class="p">:</span> <span class="n">Y_test</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">:</span><span class="mf">1.0</span><span class="p">}))</span>

        <span class="k">return</span> <span class="n">parameters</span>
</pre></div>


<h2>Run the model</h2>
<p>I run the model using the the train and validation data.</p>
<div class="highlight"><pre><span></span><span class="n">parameters</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">Y_train</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">X_val</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">Y_val</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>Cost after epoch 0: 1.115009
Cost after epoch 100: 0.030470
Cost after epoch 200: 0.013285
Cost after epoch 300: 0.011700
Cost after epoch 400: 0.008449
Cost after epoch 500: 0.006801
Cost after epoch 600: 0.005505
</pre></div>


<p><img alt="png" src="images/TensorFlow-MNIST_30_1.png"></p>
<div class="highlight"><pre><span></span>Parameters have been trained!
Train Accuracy: 1.0
Test Accuracy: 0.972262
</pre></div>


<h2>Interpretation</h2>
<p>Although the neural network is clearly overfitting the training set, by adding dropout to the layers, I could increase the accuracy from 94% to 97%. Keeping in mind, that this is a "normal", densely connected network without convolution, it is a very good result. I expect that a convolutional NN would be able to reach a higher accuracy.</p>
<h2>Make predictions and submit to kaggle</h2>
<p>...to get our performance on the untouched test set</p>
<div class="highlight"><pre><span></span><span class="n">prediction</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">prediction</span>
</pre></div>


<div class="highlight"><pre><span></span>array([2, 0, 9, ..., 3, 9, 2], dtype=int64)
</pre></div>


<div class="highlight"><pre><span></span><span class="n">submission</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;ImageId&#39;</span><span class="p">,</span><span class="s1">&#39;Label&#39;</span><span class="p">])</span>
<span class="n">submission</span><span class="p">[</span><span class="s1">&#39;ImageId&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
<span class="n">submission</span><span class="p">[</span><span class="s1">&#39;Label&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">prediction</span>
<span class="n">submission</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s1">&#39;submission3.csv&#39;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="k">print</span><span class="p">(</span><span class="s1">&#39;Result from Kaggle was: Your submission scored 0.97114&#39;</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>Result from Kaggle was: Your submission scored 0.97114
</pre></div>


<p>And that's all! I have created a 2-layered neural network that recognizes hand-written digits with a 97% accuracy (from validation and test sets).</p>
<div class="highlight"><pre><span></span>
</pre></div>
        </div>
        <div class="post_list">
            <span>By </span>
            <a href="/author/dario-arcos-diaz.html">@Dario Arcos-Díaz</a>
            <span> in </span>
            <span class="post_category"><a href="/category/machine-learning.html" rel="bookmark" title="Permalink to Machine Learning">[ Machine Learning ]</a></span>
            <span class="post_date">Sun 03 September 2017</span>
            <div><span>Tags : </span>
                <span><a href="/tag/deep-learning.html">#deep learning, </a></span>
                <span><a href="/tag/machine-learning.html">#machine learning, </a></span>
                <span><a href="/tag/neural-network.html">#neural network, </a></span>
                <span><a href="/tag/tensorflow.html">#tensorflow, </a></span>
            </div>

            <div class="entry-social">
                <span class="twitter"><a target="_blank" rel="nofollow" onclick="javascript:window.open(this.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=400,width=700');return false;" title="Twitter" href="https://twitter.com/share?url=/building-a-neural-network-from-scratch-using-tensorflow.html&text=Building a neural network from scratch using TensorFlow&via="><img src="/theme/images/icons/twitter-s.png"></a></span>

                <span class="gplus"><a target="_blank" title="Google +" href="https://plus.google.com/share?url=/building-a-neural-network-from-scratch-using-tensorflow.html&hl=fr" rel="nofollow" onclick="javascript:window.open(this.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=450,width=650');return false;"><img src="/theme/images/icons/google-s.png"></a></span>

                <span class="facebook"><a target="_blank" title="Facebook" rel="nofollow" onclick="javascript:window.open(this.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=500,width=700');return false;" href="https://www.facebook.com/sharer.php?u=/building-a-neural-network-from-scratch-using-tensorflow.html&t=Building a neural network from scratch using TensorFlow"><img src="/theme/images/icons/facebook-s.png"></a></span>

                <a  target="_blank" title="Linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=/building-a-neural-network-from-scratch-using-tensorflow.html&title=Building a neural network from scratch using TensorFlow" rel="nofollow" onclick="javascript:window.open(this.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=450,width=650');return false;"><img src="/theme/images/icons/linkedin-s.png"></a>

                <span class="mail"><a href="mailto:?subject=Building a neural network from scratch using TensorFlow&amp;body=Viens découvrir un article à propos de [Building a neural network from scratch using TensorFlow] sur le site de Dario Arcos-Díaz. /building-a-neural-network-from-scratch-using-tensorflow.html" title="Share by Email" target="_blank"><img src="/theme/images/icons/mail-s.png"></a></span>
            </div>
        </div>
    </article>
</section>
  </article>

  <!-- Footer -->
  <footer>
    <p>
      Theme <a href="https://github.com/parbhat/pelican-blue">Pelican-Blue</a> powered by <a href="http://getpelican.com/">Pelican</a>.
    </p>
  </footer>


</body>
</html>